{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T14:55:50.585995Z",
     "start_time": "2021-03-08T14:55:49.292450Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot  as plt\n",
    "from datetime import datetime\n",
    "\n",
    "from torchtext.data import get_tokenizer\n",
    "\n",
    "import random\n",
    "import collections\n",
    "\n",
    "from torchtext.transforms import Sequential, ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#from torchtext.datasets import text_classification\n",
    "# from torchtext.experimental.functional import sequential_transforms, vocab_func, totensor\n",
    "# from torchtext.experimental.datasets.text_classification import TextClassificationDataset\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset sms_spam (/home/kpapdac/.cache/huggingface/datasets/sms_spam/plain_text/1.0.0/53f051d3b5f62d99d61792c91acefe4f1577ad3e4c216fb0ad39e30b9f20019c)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd0df4ae84354e79befac5016e10975b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sms', 'label'],\n",
       "        num_rows: 5574\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we load the sms spam dataset from the huggingface hub [source: https://huggingface.co/datasets/sms_spam]\n",
    "raw_dataset = load_dataset('sms_spam')\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [tuple([raw_dataset['train']['label'][i], raw_dataset['train']['sms'][i]]) for i in range(len(raw_dataset['train']['sms']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([data[i][0] for i in range(len(raw_dataset['train']))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T14:55:51.422156Z",
     "start_time": "2021-03-08T14:55:51.385619Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Examples: 5574\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of Examples: {len(data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T14:55:51.472980Z",
     "start_time": "2021-03-08T14:55:51.424040Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_train_test_split(data, split_ratio = 0.7):\n",
    "    random.shuffle(data)\n",
    "    \n",
    "    n_train = int(len(data) * split_ratio)\n",
    "    train_data = data[:n_train]\n",
    "    test_data = data[n_train:]\n",
    "    \n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T14:55:51.525738Z",
     "start_time": "2021-03-08T14:55:51.475592Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data, test_data = get_train_test_split(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T14:55:51.575470Z",
     "start_time": "2021-03-08T14:55:51.527164Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Train Examples: 3901\n",
      "Number of Test Examples: 1673\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of Train Examples: {len(train_data)}')\n",
    "print(f'Number of Test Examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T14:55:51.623571Z",
     "start_time": "2021-03-08T14:55:51.578946Z"
    }
   },
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, tokenize_fn = 'basic_english', lower = True, max_length = None):\n",
    "        \n",
    "        self.tokenize_fn = torchtext.data.utils.get_tokenizer(tokenize_fn)\n",
    "        self.lower = lower\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def tokenize(self, s):\n",
    "        \n",
    "        tokens = self.tokenize_fn(s)\n",
    "        \n",
    "        if self.lower:\n",
    "            tokens = [token.lower() for token in tokens]\n",
    "            \n",
    "        if self.max_length is not None:\n",
    "            tokens = tokens[:self.max_length]\n",
    "            \n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T14:55:51.687334Z",
     "start_time": "2021-03-08T14:55:51.625189Z"
    }
   },
   "outputs": [],
   "source": [
    "max_length=2500\n",
    "\n",
    "tokenizer = Tokenizer(max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T14:55:51.738289Z",
     "start_time": "2021-03-08T14:55:51.689032Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham', 'but', 'not']\n"
     ]
    }
   ],
   "source": [
    "s = 'ham but not'\n",
    "print(tokenizer.tokenize(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T14:55:51.787378Z",
     "start_time": "2021-03-08T14:55:51.740293Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_vocab_from_data(data, tokenizer, **vocab_kwarg):\n",
    "    \n",
    "    token_freqs = collections.Counter()\n",
    "    \n",
    "    for label, text  in data:\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        token_freqs.update(tokens)\n",
    "        \n",
    "    vocab = torchtext.vocab.Vocab(token_freqs, **vocab_kwarg)\n",
    "    \n",
    "    return vocab, token_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T14:55:51.920403Z",
     "start_time": "2021-03-08T14:55:51.788795Z"
    }
   },
   "outputs": [],
   "source": [
    "max_size = 25000\n",
    "\n",
    "vocab, token_freqs = build_vocab_from_data(train_data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T14:55:51.939507Z",
     "start_time": "2021-03-08T14:55:51.921953Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words in vocab: 7619\n"
     ]
    }
   ],
   "source": [
    "print(f'Unique words in vocab: {len(vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T14:55:51.992316Z",
     "start_time": "2021-03-08T14:55:51.944041Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 7994), ('i', 2086), ('to', 1602), ('you', 1533), (',', 1376)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_freqs.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=8, shuffle=False, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_data, batch_size=8, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class TextClassificationModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_data), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x)\n",
    "\n",
    "num_class = len(set([label for (label, text) in train_data]))\n",
    "vocab_size = len(vocab)\n",
    "emsize = 64\n",
    "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text, offsets)\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
    "                                              total_acc/total_count))\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            predicted_label = model(text, offsets)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time:  0.61s | valid accuracy    0.934 \n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time:  0.19s | valid accuracy    0.964 \n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time:  0.19s | valid accuracy    0.964 \n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time:  0.19s | valid accuracy    0.969 \n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time:  0.19s | valid accuracy    0.969 \n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time:  0.21s | valid accuracy    0.969 \n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time:  0.19s | valid accuracy    0.974 \n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time:  0.21s | valid accuracy    0.969 \n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time:  0.19s | valid accuracy    0.969 \n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time:  0.21s | valid accuracy    0.969 \n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "# Hyperparameters\n",
    "EPOCHS = 10 # epoch\n",
    "LR = 5  # learning rate\n",
    "BATCH_SIZE = 64 # batch size for training\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "total_accu = None\n",
    "train_dataset = to_map_style_dataset(train_data)\n",
    "test_dataset = to_map_style_dataset(test_data)\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ = \\\n",
    "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
    "\n",
    "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                             shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader)\n",
    "    accu_val = evaluate(valid_dataloader)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        total_accu = accu_val\n",
    "    print('-' * 59)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "          'valid accuracy {:8.3f} '.format(epoch,\n",
    "                                           time.time() - epoch_start_time,\n",
    "                                           accu_val))\n",
    "    print('-' * 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the results of test dataset.\n",
      "test accuracy    0.977\n"
     ]
    }
   ],
   "source": [
    "print('Checking the results of test dataset.')\n",
    "accu_test = evaluate(test_dataloader)\n",
    "print('test accuracy {:8.3f}'.format(accu_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a spam news\n"
     ]
    }
   ],
   "source": [
    "ag_news_label = {0: \"ham\",\n",
    "                 1: \"spam\"}\n",
    "\n",
    "def predict(text, text_pipeline):\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor(text_pipeline(text))\n",
    "        output = model(text, torch.tensor([0]))\n",
    "        return output.argmax(1).item() + 1\n",
    "\n",
    "ex_text_str = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n",
    "    enduring the season’s worst weather conditions on Sunday at The \\\n",
    "    Open on his way to a closing 75 at Royal Portrush, which \\\n",
    "    considering the wind and the rain was a respectable showing. \\\n",
    "    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n",
    "    was another story. With temperatures in the mid-80s and hardly any \\\n",
    "    wind, the Spaniard was 13 strokes better in a flawless round. \\\n",
    "    Thanks to his best putting performance on the PGA Tour, Rahm \\\n",
    "    finished with an 8-under 62 for a three-stroke lead, which \\\n",
    "    was even more impressive considering he’d never played the \\\n",
    "    front nine at TPC Southwind.\"\n",
    "\n",
    "model = model.to(\"cpu\")\n",
    "\n",
    "print(\"This is a %s news\" %ag_news_label[predict(ex_text_str, text_pipeline)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T14:55:52.549782Z",
     "start_time": "2021-03-08T14:55:52.503926Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, n_vocab, embed_dim, n_hidden, n_rnnlayers, n_outputs):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.V = n_vocab\n",
    "        self.D = embed_dim\n",
    "        self.M = n_hidden\n",
    "        self.K = n_outputs\n",
    "        self.L = n_rnnlayers\n",
    "\n",
    "        self.embed = nn.Embedding(self.V, self.D)\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=self.D,\n",
    "            hidden_size=self.M,\n",
    "            num_layers=self.L,\n",
    "            batch_first=True)\n",
    "        self.fc = nn.Linear(self.M, self.K)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        h0 = torch.zeros(self.L, X.size(0), self.M).to(device)\n",
    "        c0 = torch.zeros(self.L, X.size(0), self.M).to(device)\n",
    "\n",
    "\n",
    "        out = self.embed(X)\n",
    "\n",
    "        # get RNN unit output\n",
    "        out, _ = self.rnn(out, (h0, c0))\n",
    "\n",
    "        # max pool\n",
    "        out, _ = torch.max(out, 1)\n",
    "\n",
    "        # we only want h(T) at the final time step\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T14:55:52.612604Z",
     "start_time": "2021-03-08T14:55:52.551894Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T14:55:52.665605Z",
     "start_time": "2021-03-08T14:55:52.614710Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (embed): Embedding(7602, 20)\n",
       "  (rnn): LSTM(20, 15, batch_first=True)\n",
       "  (fc): Linear(in_features=15, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTM(len(vocab), 20, 15, 1, 1)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T14:55:52.713661Z",
     "start_time": "2021-03-08T14:55:52.668608Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T14:55:52.763928Z",
     "start_time": "2021-03-08T14:55:52.715593Z"
    }
   },
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self,\n",
    "                model,\n",
    "                optim,\n",
    "                loss_fn,\n",
    "                device=\"cpu\"):\n",
    "        self.model = model\n",
    "        self.optim = optim\n",
    "        self.loss_fn = loss_fn\n",
    "        self.device = device\n",
    "        \n",
    "    def fit(self,\n",
    "            train_loader,\n",
    "            test_loader,\n",
    "            n_epochs: int=100,\n",
    "            eval_inter: int=1):\n",
    "        history = {}\n",
    "        history['epoch'] = []\n",
    "        history['training_acc'] = []\n",
    "        history['test_acc'] = []\n",
    "        history['training_loss'] = []\n",
    "        history['test_loss'] = []\n",
    "        # Training Loop\n",
    "\n",
    "        history['p_test'] = []\n",
    "        history['y_test'] = []\n",
    "        p_test = np.array([]) # for listing all predictions - last epoch only\n",
    "        y_test = np.array([]) # for listing all predictions - last epoch only\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            history['epoch'].append(epoch+1)\n",
    "            \n",
    "            self.model.train()\n",
    "            \n",
    "            train_loss = []\n",
    "            n_correct = 0.\n",
    "            n_total = 0.\n",
    "            for inputs, targets in tqdm(train_loader, leave=False):\n",
    "                targets = targets.view(-1, 1).float()\n",
    "                # Move data to GPU\n",
    "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "\n",
    "                # zero the gradient\n",
    "                self.optim.zero_grad()\n",
    "\n",
    "                # forward pass\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.loss_fn(outputs, targets)\n",
    "\n",
    "                # get prediction\n",
    "                predictions = (outputs.detach().numpy() > 0) # for bcewithlogitsloss\n",
    "\n",
    "                # update counts\n",
    "                n_correct += (predictions == targets.numpy()).sum().item()\n",
    "                n_total += targets.shape[0]\n",
    "\n",
    "                # backward pass and optimize\n",
    "                loss.backward()\n",
    "                self.optim.step()\n",
    "\n",
    "                train_loss.append(loss.item())\n",
    "\n",
    "            train_loss = np.mean(train_loss)\n",
    "            history['training_acc'].append(n_correct / n_total * 100) \n",
    "            # save losses\n",
    "            history['training_loss'].append(train_loss) \n",
    "    \n",
    "           \n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                test_loss = []\n",
    "                n_correct = 0.\n",
    "                n_total = 0.\n",
    "                for inputs, targets in test_loader:\n",
    "                    targets = targets.view(-1, 1).float()\n",
    "                    # Move data to GPU\n",
    "                    inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "\n",
    "                    # forward pass\n",
    "                    outputs_test = self.model(inputs)\n",
    "                    loss_test = self.loss_fn(outputs_test, targets)\n",
    "\n",
    "                    # get prediction\n",
    "                    predictions = (outputs_test.numpy() > 0) # for bcewithlogitsloss\n",
    "        \n",
    "                    # update list of predictions  - done for last epoch only!\n",
    "                    if epoch == (n_epochs - 1):\n",
    "                        p_test = np.concatenate((p_test, predictions.flatten()), axis=0)\n",
    "                        y_test = np.concatenate((y_test, targets.flatten()), axis=0)\n",
    "\n",
    "                    # update counts\n",
    "                    n_correct += (targets.numpy() == predictions).sum().item()\n",
    "                    n_total += targets.shape[0]\n",
    "\n",
    "                    test_loss.append(loss_test.item())\n",
    "            history['p_test'] = p_test\n",
    "            history['y_test'] = y_test\n",
    "            test_loss = np.mean(test_loss)\n",
    "            history['test_acc'].append(n_correct / n_total * 100) \n",
    "            # save losses\n",
    "            history['test_loss'].append(test_loss) \n",
    "            \n",
    "            if (epoch +1) % eval_inter == 0:\n",
    "                print(f\"Epoch: {epoch+1}/{n_epochs}, Train Accuracy: {history['training_acc'][-1]:.2f}%, Test Accuracy: {history['test_acc'][-1]:.2f}%, Train Loss: {history['training_loss'][-1]:.4f}, Test Loss: {history['test_loss'][-1]:.4f}\", end='')\n",
    "        return history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T14:56:32.208300Z",
     "start_time": "2021-03-08T14:55:52.765447Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f378b126a1c48c78b189afb9f6e1686",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/ipykernel_21903/3423399829.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/tmp/ipykernel_21903/2489595098.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_loader, test_loader, n_epochs, eval_inter)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mn_correct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mn_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m                 \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0;31m# Move data to GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model, optimizer, loss_fn,device)\n",
    "history={}\n",
    "history = trainer.fit(train_dataloader,test_dataloader,15,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-08T14:57:31.052704Z",
     "start_time": "2021-03-08T14:57:30.675070Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Set font scale\n",
    "sns.set(font_scale=1.5)\n",
    "\n",
    "# Create a confusion matrix\n",
    "conf_mat = confusion_matrix(history['y_test'], history['p_test'])\n",
    "\n",
    "\n",
    "def plot_conf_mat_sb(conf_mat):\n",
    "    \"\"\"\n",
    "    Plots a confusion matrix using seaborn's heatmap\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(3,3))\n",
    "    ax = sns.heatmap(conf_mat,\n",
    "                    annot=True,\n",
    "                    cbar=False)\n",
    "    \n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "\n",
    "plot_conf_mat_sb(conf_mat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
